{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from face_detector_image_generator import face_detection_and_capture\n",
    "from nan_finder import find_nan_columns\n",
    "from keypoints_display_df import show_multiple_images_with_keypoints_in_dataframe\n",
    "from keypoints_display_tf import show_multiple_images_with_keypoints_in_tf_datasets\n",
    "from keypoints_flip_tf import flip_coordinate_and_image_horizontal_in_tf_dataset, flip_coordinate_and_image_vertical_in_tf_dataset\n",
    "from keypoints_flip_df import flip_coordinate_and_image_horizontal_in_df, flip_coordinate_and_image_vertical_in_df\n",
    "from sobel_module import tf_compute_sobel\n",
    "from canny_module import tf_compute_canny\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deteksi Titik Kunci pada Wajah Menggunakan Jaringan Saraf Tiruan**\n",
    "---\n",
    "\n",
    "Setelah kita belajar mendeteksi wajah menggunakan metode Viola-Jones, langkah berikutnya adalah mendeteksi titik-titik kunci pada wajah menggunakan jaringan saraf tiruan (neural network). Titik-titik kunci ini bisa mencakup posisi mata, hidung, mulut, dan sebagainya.\n",
    "\n",
    "#### Apa Itu Deteksi Titik Kunci Wajah?\n",
    "\n",
    "**Deteksi titik kunci wajah** adalah proses menemukan titik-titik spesifik pada wajah, seperti ujung mata, sudut bibir, atau ujung hidung. Ini lebih rumit dibandingkan deteksi objek biasa karena kita harus menemukan banyak titik pada berbagai posisi dan orientasi di wajah, bukan hanya empat sudut dari kotak pembatas.\n",
    "\n",
    "#### Langkah-langkah dalam Deteksi Titik Kunci\n",
    "\n",
    "1. **Deteksi Wajah**: Pertama-tama, kita harus mendeteksi wajah dalam gambar, biasanya dengan menempatkannya dalam sebuah kotak pembatas (bounding box).\n",
    "  \n",
    "2. **Deteksi Titik Kunci**: Setelah wajah ditemukan, kita menggunakan jaringan saraf tiruan untuk mendeteksi titik-titik kunci pada wajah tersebut. Ini berarti kita harus menemukan lebih dari 10 titik berbeda yang merepresentasikan bagian-bagian penting dari wajah.\n",
    "\n",
    "#### Menggunakan Data untuk Pelatihan\n",
    "\n",
    "Untuk melatih jaringan saraf tiruan agar bisa mendeteksi titik kunci dengan baik, kita memerlukan banyak data. Salah satu sumber data yang bagus adalah dari Kaggle, di mana terdapat banyak gambar wajah beserta titik-titik kuncinya. Misalnya, tantangan deteksi titik kunci wajah di Kaggle menyediakan file CSV yang berisi link ke 7.049 gambar (berukuran 96 x 96 piksel), masing-masing dengan 15 titik kunci.\n",
    "\n",
    "#### Proses Pelatihan\n",
    "\n",
    "Untuk melatih model deteksi titik kunci:\n",
    "1. **Kumpulkan Data**: Kumpulkan gambar wajah dan tandai titik-titik kuncinya.\n",
    "2. **Persiapkan Data**: Format data tersebut agar bisa digunakan oleh jaringan saraf tiruan.\n",
    "3. **Latih Model**: Gunakan data tersebut untuk melatih model jaringan saraf tiruan agar dapat mendeteksi titik-titik kunci pada wajah baru.\n",
    "\n",
    "#### Mengapa Ini Penting?\n",
    "\n",
    "Deteksi titik kunci pada wajah sangat berguna dalam berbagai aplikasi, seperti:\n",
    "- **Pengenalan Wajah**: Membantu komputer mengenali siapa orang dalam gambar.\n",
    "- **Animasi Wajah**: Digunakan dalam pembuatan karakter animasi yang realistis.\n",
    "- **Analisis Ekspresi Wajah**: Membantu memahami emosi seseorang dari ekspresi wajahnya.\n",
    "\n",
    "#### Kesimpulan\n",
    "\n",
    "Deteksi titik kunci pada wajah adalah langkah lanjutan setelah deteksi wajah yang memungkinkan kita untuk menganalisis wajah dengan lebih detail. Dengan menggunakan jaringan saraf tiruan, kita bisa mendeteksi titik-titik kunci ini secara akurat, meskipun ini adalah tugas yang lebih rumit dibandingkan deteksi objek biasa.\n",
    "\n",
    "Semoga penjelasan ini membantu! Jika ada yang masih membingungkan atau ada yang ingin ditanyakan lebih lanjut, jangan ragu untuk bertanya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mempersiapkan Dataset untuk Deteksi Titik Kunci Wajah**\n",
    "---\n",
    "\n",
    "Untuk melatih komputer agar bisa mendeteksi titik kunci pada wajah, kita perlu mengumpulkan dan mempersiapkan dataset yang berisi banyak gambar wajah. Berikut adalah langkah-langkah untuk membuat dataset ini menggunakan webcam di komputer kamu.\n",
    "\n",
    "### Langkah-langkah Membuat Dataset\n",
    "\n",
    "1. **Memuat Haar Cascade Classifier**\n",
    "   - Pertama, kita perlu memuat classifier Haar cascade yang akan kita gunakan untuk mendeteksi wajah. Classifier ini adalah file XML yang bisa kamu temukan di folder OpenCV kamu.\n",
    "   - Contoh kode:\n",
    "     ```python\n",
    "     face_cascade = cv2.CascadeClassifier('path_to_haarcascade_frontalface_default.xml')\n",
    "     ```\n",
    "\n",
    "2. **Mengatur Kamera**\n",
    "   - Kita menggunakan webcam untuk mengambil gambar wajah. Perintah `cv2.VideoCapture(0)` digunakan untuk mengakses kamera internal komputer. Jika kamu menggunakan kamera eksternal, gunakan `cv2.VideoCapture(1)`.\n",
    "   - Contoh kode:\n",
    "     ```python\n",
    "     cam = cv2.VideoCapture(0)\n",
    "     ```\n",
    "\n",
    "3. **Mendeteksi Wajah dan Menampilkan di Layar**\n",
    "   - Setiap frame yang dibaca dari kamera diproses untuk mendeteksi wajah menggunakan classifier yang kita muat di langkah 1. Jika wajah terdeteksi, kotak pembatas akan digambar di sekitar wajah dan hanya bagian wajah yang akan ditampilkan di layar.\n",
    "   - Contoh kode:\n",
    "     ```python\n",
    "     while(True):\n",
    "         ret, frame = cam.read()\n",
    "         faces = face_cascade.detectMultiScale(frame, 1.3, 5)\n",
    "         for (x, y, w, h) in faces:\n",
    "             if w > 130:\n",
    "                 detected_face = frame[int(y):int(y+h), int(x):int(x+w)]\n",
    "                 cv2.imshow(\"test\", detected_face)\n",
    "         if not ret:\n",
    "             break\n",
    "         k = cv2.waitKey(1)\n",
    "     ```\n",
    "\n",
    "4. **Menyimpan Gambar**\n",
    "   - Gambar wajah yang terdeteksi kemudian diubah ukurannya menjadi 299x299 piksel dan disimpan dalam folder yang sudah kamu buat (misalnya, folder `dataset`). Setiap kali kamu menekan tombol spasi, gambar akan disimpan dengan nomor file yang terus bertambah.\n",
    "   - Contoh kode:\n",
    "     ```python\n",
    "     faceresize = cv2.resize(detected_face, (299, 299))\n",
    "     img_name = \"dataset/opencv_frame_{}.jpg\".format(img_counter)\n",
    "     cv2.imwrite(img_name, faceresize)\n",
    "     ```\n",
    "\n",
    "5. **Mengumpulkan dan Mengannotasi Gambar**\n",
    "   - Ambil sekitar 100 gambar wajah dengan berbagai posisi dan orientasi. Lebih banyak gambar akan memberikan hasil deteksi yang lebih baik. Untuk anotasi (penandaan) titik kunci pada wajah, kamu bisa menggunakan alat anotasi seperti VGG annotator.\n",
    "   - VGG annotator memungkinkan kamu untuk menandai titik-titik kunci seperti mata, hidung, dan bibir pada gambar. Misalnya, kamu bisa menandai 16 titik pada wajah yang mencakup mata kiri, mata kanan, hidung, bibir, dan bentuk wajah luar.\n",
    "\n",
    "### Kesimpulan\n",
    "\n",
    "Dengan mengikuti langkah-langkah ini, kamu bisa membuat dataset sendiri yang berisi gambar-gambar wajah dengan titik-titik kunci yang telah dianotasi. Dataset ini sangat penting untuk melatih model jaringan saraf tiruan agar bisa mendeteksi titik-titik kunci pada wajah dengan akurat. Proses ini memerlukan kerja keras dan ketelitian, tetapi hasilnya akan sangat bermanfaat untuk berbagai aplikasi seperti pengenalan wajah dan analisis ekspresi wajah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face_cascade_path = './Assets/data/haarcascades/haarcascade_frontalface_default.xml'\n",
    "# face_detection_and_capture(face_cascade_path=face_cascade_path, output_dir='./Assets/Images/', img_size=(299, 299), alpha=0.1, offset=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Memproses Data Titik Kunci**\n",
    "---\n",
    "\n",
    "Ketika kita ingin mendeteksi titik-titik penting di wajah (seperti mata, hidung, dan mulut), kita membutuhkan data yang sudah di-annotate. Ada alat yang disebut VGG annotator yang bisa membantu kita dengan ini. Alat ini akan menghasilkan file CSV yang berisi koordinat (x, y) untuk setiap titik kunci pada setiap gambar.\n",
    "\n",
    "#### Mengapa Ini Penting?\n",
    "\n",
    "1. **Efisiensi**: Python kita tidak akan mencari banyak file gambar secara langsung, tetapi akan mencari data dari file CSV.\n",
    "2. **Pengolahan Data**: Untuk setiap file CSV, ada 16 titik kunci yang harus diproses.\n",
    "3. **Alternatif**: Ini adalah alternatif untuk menggunakan metode `ImageDataGenerator` dari Keras yang biasa digunakan untuk mengelola banyak file dalam direktori.\n",
    "\n",
    "#### Langkah-Langkahnya\n",
    "\n",
    "Untuk memudahkan pemahaman, kita akan membagi proses ini menjadi dua bagian:\n",
    "\n",
    "1. **Praproses sebelum input ke dalam kode Keras-Python**\n",
    "2. **Praproses dalam kode Keras-Python**\n",
    "\n",
    "#### Praproses Sebelum Input ke Keras-Python\n",
    "\n",
    "Sebelum kita memasukkan data ke dalam model Keras, kita perlu membersihkan dan menyiapkan data dari file CSV yang dihasilkan oleh VGG annotator. Proses ini melibatkan membaca file CSV, mengekstrak koordinat titik kunci, dan memastikan bahwa data siap untuk digunakan oleh model.\n",
    "\n",
    "#### Praproses Dalam Kode Keras-Python\n",
    "\n",
    "Setelah data siap, kita akan menggunakan Keras (sebuah pustaka untuk membuat dan melatih model pembelajaran mendalam) untuk memproses data lebih lanjut. Di sini, kita akan memastikan bahwa data diolah dengan benar saat dilatih oleh model.\n",
    "\n",
    "#### Kesimpulan\n",
    "\n",
    "Memproses data titik kunci memerlukan dua langkah utama: praproses sebelum memasukkan data ke dalam model Keras, dan praproses dalam model Keras itu sendiri. Dengan cara ini, kita memastikan bahwa data yang kita gunakan bersih, terstruktur, dan siap digunakan untuk melatih model pembelajaran mendalam yang akurat dan efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './Assets/datasets/training.csv'\n",
    "test_path = './Assets/datasets/test.csv'\n",
    "\n",
    "# train_path = './Assets/trainimgface.csv'\n",
    "# test_path = './Assets/testimgface.csv'\n",
    "train_data = pd.read_csv(train_path)  \n",
    "test_data = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nan_in_train_data = find_nan_columns(train_data, name_columns='facial_key_points', max_display=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_columns = train_data.columns[:-1]\n",
    "show_multiple_images_with_keypoints_in_dataframe(\n",
    "    train_data, \n",
    "    image_column='Image',\n",
    "    # parent_path='./Assets/Images/',\n",
    "    keypoints_columns=keypoints_columns, \n",
    "    num_images=6, \n",
    "    image_size=(96, 96),\n",
    "    figsize_per_image=(2, 2),\n",
    "    scatter_size=100,\n",
    "    n_seed=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_copy.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nan_in_train_data_copy = find_nan_columns(train_data_copy, name_columns='facial_key_points', max_display=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "data: {train_data_copy['Image'][0][:10]}\n",
    "len: {len(train_data_copy['Image'][0][:10])}\n",
    "type: {type(train_data_copy['Image'][0][:10])}\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = len(train_data_copy['Image'][[0]].str.split(' ')[0])\n",
    "target_reshape = int(np.sqrt(length_data))\n",
    "\n",
    "print(f'''\n",
    "length data: {length_data}\n",
    "target_reshape: {target_reshape}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_images=train_data_copy['Image'].values\n",
    "train_data_facial_keypoints=train_data_copy.drop(columns=['Image']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_keypoints_train_datasets = tf.data.Dataset.from_tensor_slices((train_data_images, train_data_facial_keypoints))\n",
    "\n",
    "print(f'info data: {facial_keypoints_train_datasets}')\n",
    "print(f'number of data: {len(facial_keypoints_train_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, keypoint in facial_keypoints_train_datasets.take(1).as_numpy_iterator():\n",
    "    print(f\"{'Check data'.center(61, '=')}\")\n",
    "    print(f'''    img value: {img[:20]}\n",
    "    dtype img: {type(img)}\n",
    "    keypoint value: {keypoint}\n",
    "    dtype keypoint: {keypoint.dtype}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Convert to List'.center(61, '=')}\")\n",
    "    cvt_to_list = tf.strings.split(img, sep=' ')\n",
    "    print(f'''    vector of img: {cvt_to_list} \n",
    "    shape img: {cvt_to_list.shape}\n",
    "    dtype img: {cvt_to_list.dtype}'''\n",
    "    )\n",
    "    \n",
    "    print(f\"{'Convert to Numeric'.center(61, '=')}\")\n",
    "    cvt_to_num = image = tf.strings.to_number(cvt_to_list, out_type=tf.float32)\n",
    "    print(f'''    vector of img: {cvt_to_num} \n",
    "    shape img: {cvt_to_num.shape}\n",
    "    dtype img: {cvt_to_num.dtype}\n",
    "    target image ndim: {tf.math.sqrt(cvt_to_num.shape[0] / 1.)}\n",
    "    max intensity: {tf.reduce_max(cvt_to_num)}\n",
    "    min intensity: {tf.reduce_min(cvt_to_num)}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Reshape to height and width dimension'.center(61, '=')}\")\n",
    "    cvt_to_img = tf.reshape(cvt_to_num, (96, 96))\n",
    "    print(f'''    matriks of img: {cvt_to_img} \n",
    "    shape img: {cvt_to_img.shape}\n",
    "    dtype img: {cvt_to_img.dtype}\n",
    "    max intensity: {tf.reduce_max(cvt_to_img)}\n",
    "    min intensity: {tf.reduce_min(cvt_to_img)}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Normalized image'.center(61, '=')}\")\n",
    "    normalized_img = (cvt_to_img / 255) * 1.0\n",
    "    print(f'''    matriks of img: {normalized_img} \n",
    "    shape img: {normalized_img.shape}\n",
    "    dtype img: {normalized_img.dtype}\n",
    "    max intensity: {tf.reduce_max(normalized_img)}\n",
    "    min intensity: {tf.reduce_min(normalized_img)}'''\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(keypoint), 2):\n",
    "        x = keypoint[i]\n",
    "        y = keypoint[i+1]\n",
    "        plt.scatter(x, y, s=75, marker='.', c='blue')\n",
    "    plt.imshow(normalized_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_train_data(image, keypoint=None, input_shape=None):\n",
    "    image = tf.strings.split(image, sep=' ')\n",
    "    image = tf.strings.to_number(image, out_type=tf.float32)\n",
    "    image = (image / 255) * 1.0\n",
    "    image = tf.reshape(image, input_shape)\n",
    "    return image, keypoint\n",
    "\n",
    "def replace_nan_with_zero(image, keypoint):\n",
    "    keypoint = tf.where(tf.math.is_nan(keypoint), tf.zeros_like(keypoint), keypoint)\n",
    "    return image, keypoint\n",
    "\n",
    "def get_nan_in_data(image, keypoint):\n",
    "    # Memeriksa apakah ada NaN dalam keypoint\n",
    "    get_nan_in_data = tf.reduce_any(tf.math.is_nan(keypoint))\n",
    "    return get_nan_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nan_infacial_keypoints_train_datasets = facial_keypoints_train_datasets.filter(get_nan_in_data)\n",
    "\n",
    "print(get_nan_infacial_keypoints_train_datasets)\n",
    "for image, keypoint in get_nan_infacial_keypoints_train_datasets.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Keypoints:\", keypoint.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canny = Canny(sigma=0.9, threshold_min=50, threshold_max=100, tracking_con=5, tracking_iterations=8)\n",
    "\n",
    "facial_keypoints_train_datasets_processed = facial_keypoints_train_datasets.map(\n",
    "    map_func=lambda image, keypoint: \n",
    "        preprocessing_train_data(\n",
    "            image=image, \n",
    "            keypoint=keypoint,\n",
    "            input_shape=(96, 96, 1)\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "facial_keypoints_train_datasets_flip_h = facial_keypoints_train_datasets_processed.map(\n",
    "    map_func=lambda image, keypoint: \n",
    "        flip_coordinate_and_image_horizontal_in_tf_dataset(\n",
    "            image=image,\n",
    "            keypoint=keypoint\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "facial_keypoints_concatenated_train_dataset = facial_keypoints_train_datasets_processed.concatenate(facial_keypoints_train_datasets_flip_h)\n",
    "\n",
    "facial_keypoints_train_datasets_not_nan = facial_keypoints_concatenated_train_dataset.map(\n",
    "    map_func=lambda image, keypoint: \n",
    "        replace_nan_with_zero(\n",
    "            image=image, \n",
    "            keypoint=keypoint,\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "facial_keypoints_concatenated_train_dataset_cached = facial_keypoints_train_datasets_not_nan.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'info data: {facial_keypoints_concatenated_train_dataset_cached}')\n",
    "print(f'number of data: {len(facial_keypoints_concatenated_train_dataset_cached)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, keypoint in facial_keypoints_concatenated_train_dataset_cached.skip(80).take(1).as_numpy_iterator():\n",
    "    print(f\"{'Check data'.center(61, '=')}\")\n",
    "    print(f'''    shape img: {img.shape}\n",
    "    dtype img: {img.dtype}\n",
    "    max intensity: {tf.reduce_max(img)}\n",
    "    min intensity: {tf.reduce_min(img)}\n",
    "    keypoint value: {keypoint}\n",
    "    dtype keypoint: {keypoint.dtype}'''\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(keypoint), 2):\n",
    "        x = keypoint[i]\n",
    "        y = keypoint[i+1]\n",
    "        plt.scatter(x, y, s=75, marker='.', c='blue')\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nan_infacial_keypoints_train_datasets = facial_keypoints_concatenated_train_dataset_cached.filter(get_nan_in_data)\n",
    "\n",
    "print(get_nan_infacial_keypoints_train_datasets)\n",
    "for image, keypoint in get_nan_infacial_keypoints_train_datasets.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Keypoints:\", keypoint.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original Datasets')\n",
    "tf.random.set_seed(12)\n",
    "show_multiple_images_with_keypoints_in_tf_datasets(\n",
    "    dataset=facial_keypoints_concatenated_train_dataset_cached.take(int(len(facial_keypoints_concatenated_train_dataset_cached)/2)), \n",
    "    num_images=12, \n",
    "    figsize_per_image=(2, 2), \n",
    "    scatter_size=10, \n",
    "    scatter_color='blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fliped Datasets')\n",
    "tf.random.set_seed(12)\n",
    "show_multiple_images_with_keypoints_in_tf_datasets(\n",
    "    dataset=facial_keypoints_concatenated_train_dataset_cached.skip(int(len(facial_keypoints_concatenated_train_dataset_cached)/2)), \n",
    "    num_images=12, \n",
    "    figsize_per_image=(2, 2), \n",
    "    scatter_size=10, \n",
    "    scatter_color='blue'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facial_keypoints_concatenated_train_dataset_cached.save(\"./Assets/datasets/facial_keypoints_train_dataset.tfrecord\", compression=\"GZIP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_copy = test_data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_keypoints_test_datasets = tf.data.Dataset.from_tensor_slices(test_data_copy['Image'].values)\n",
    "\n",
    "print(f'info data: {facial_keypoints_test_datasets}')\n",
    "print(f'number of data: {len(facial_keypoints_test_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in facial_keypoints_test_datasets.take(1).as_numpy_iterator():\n",
    "    print(f\"{'Check data'.center(61, '=')}\")\n",
    "    print(f'''    img value: {img[:20]}\n",
    "    dtype img: {type(img)}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Convert to List'.center(61, '=')}\")\n",
    "    cvt_to_list = tf.strings.split(img, sep=' ')\n",
    "    print(f'''    vector of img: {cvt_to_list} \n",
    "    shape img: {cvt_to_list.shape}\n",
    "    dtype img: {cvt_to_list.dtype}'''\n",
    "    )\n",
    "    \n",
    "    print(f\"{'Convert to Numeric'.center(61, '=')}\")\n",
    "    cvt_to_num = image = tf.strings.to_number(cvt_to_list, out_type=tf.float32)\n",
    "    print(f'''    vector of img: {cvt_to_num} \n",
    "    shape img: {cvt_to_num.shape}\n",
    "    dtype img: {cvt_to_num.dtype}\n",
    "    target image ndim: {tf.math.sqrt(cvt_to_num.shape[0] / 1.)}\n",
    "    max intensity: {tf.reduce_max(cvt_to_num)}\n",
    "    min intensity: {tf.reduce_min(cvt_to_num)}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Reshape to height and width dimension'.center(61, '=')}\")\n",
    "    cvt_to_img = tf.reshape(cvt_to_num, (96, 96))\n",
    "    print(f'''    matriks of img: {cvt_to_img} \n",
    "    shape img: {cvt_to_img.shape}\n",
    "    dtype img: {cvt_to_img.dtype}\n",
    "    max intensity: {tf.reduce_max(cvt_to_img)}\n",
    "    min intensity: {tf.reduce_min(cvt_to_img)}'''\n",
    "    )\n",
    "\n",
    "    print(f\"{'Normalized image'.center(61, '=')}\")\n",
    "    normalized_img = (cvt_to_img / 255) * 1.0\n",
    "    print(f'''    matriks of img: {normalized_img} \n",
    "    shape img: {normalized_img.shape}\n",
    "    dtype img: {normalized_img.dtype}\n",
    "    max intensity: {tf.reduce_max(normalized_img)}\n",
    "    min intensity: {tf.reduce_min(normalized_img)}'''\n",
    "    )\n",
    "\n",
    "    for i in range(0, len(keypoint), 2):\n",
    "        x = keypoint[i]\n",
    "        y = keypoint[i+1]\n",
    "        plt.scatter(x, y, s=75, marker='.', c='blue')\n",
    "    plt.imshow(normalized_img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_test_data(image, input_shape=None):\n",
    "    # Split the image string into a list of numbers\n",
    "    image = tf.strings.split(image, sep=' ')\n",
    "    # Convert the split strings into numbers\n",
    "    image = tf.strings.to_number(image, out_type=tf.float32)\n",
    "    image = (image / 255) * 1.0\n",
    "    # Reshape the image to its original shape (e.g., 96x96 if the image is 96x96 pixels)\n",
    "    image = tf.reshape(image, input_shape)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_keypoints_test_datasets_processed = facial_keypoints_test_datasets.map(\n",
    "    map_func=lambda image: \n",
    "        preprocessing_test_data(\n",
    "            image=image, \n",
    "            input_shape=(96, 96, 1)\n",
    "        ),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "facial_keypoints_test_dataset_cached = facial_keypoints_test_datasets_processed.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'info data: {facial_keypoints_test_dataset_cached}')\n",
    "print(f'number of data: {len(facial_keypoints_test_dataset_cached)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in facial_keypoints_test_dataset_cached.skip(1).take(1).as_numpy_iterator():\n",
    "    print(f\"{'Check data'.center(61, '=')}\")\n",
    "    print(f'''    shape img: {img.shape}\n",
    "    dtype img: {img.dtype}\n",
    "    max intensity: {tf.reduce_max(img)}\n",
    "    min intensity: {tf.reduce_min(img)}\n",
    "    keypoint value: {keypoint}\n",
    "    dtype keypoint: {keypoint.dtype}'''\n",
    "    )\n",
    "    plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_multiple_images_with_keypoints_in_tf_datasets(facial_keypoints_test_dataset_cached.map(lambda x: (x, None)), num_images=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facial_keypoints_test_dataset_cached.save(\"./Assets/datasets/facial_keypoints_test_dataset.tfrecord\", compression=\"GZIP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
