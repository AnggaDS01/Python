{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 20, 30, 40],\n",
      "        [50, 60, 70, 80]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]]) \n",
    "print(x * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 12, 13, 14],\n",
      "        [15, 16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]]) \n",
    "y = x + 10\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: tensor([2, 3, 1, 0]), shape: torch.Size([4])\n",
      "value: tensor([[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]), shape: torch.Size([4, 1])\n",
      "value: tensor([[2],\n",
      "        [3],\n",
      "        [1],\n",
      "        [0]]), shape: torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.tensor([2, 3, 1, 0]) # y.shape == (4)\n",
    "print(f'value: {y}, shape: {y.shape}')\n",
    "\n",
    "y = y.view(4, 1)                # y.shape == (4, 1)\n",
    "print(f'value: {y}, shape: {y.shape}')\n",
    "\n",
    "y = y.reshape(4, 1)\n",
    "print(f'value: {y}, shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.6784],\n",
       "         [-0.9309],\n",
       "         [ 0.2471]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(size=(1, 3, 1))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6784, -0.9309,  0.2471])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11],\n",
      "        [35]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8]])\n",
    "print(torch.matmul(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7, 6],\n",
       "         [8, 5],\n",
       "         [1, 3]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(seed=42)\n",
    "x = torch.randint(low=1, high=10, size=(1, 3, 2))\n",
    "x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 15,  9, 19],\n",
      "        [13, 13, 14, 10],\n",
      "        [12, 13, 13,  5]], dtype=torch.int32)\n",
      "tensor([[19],\n",
      "        [14],\n",
      "        [13]], dtype=torch.int32)\n",
      "tensor([[3],\n",
      "        [2],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(12)\n",
    "A = torch.randint(low=1, high=20, size=(3, 4), dtype=torch.int32)\n",
    "print(A)\n",
    "\n",
    "get_max = A.max(axis=-1, keepdim=True)\n",
    "print(get_max.values)\n",
    "print(get_max.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "reshape(...)\n",
      "    reshape(*shape) -> Tensor\n",
      "    \n",
      "    Returns a tensor with the same data and number of elements as :attr:`self`\n",
      "    but with the specified shape. This method returns a view if :attr:`shape` is\n",
      "    compatible with the current shape. See :meth:`torch.Tensor.view` on when it is\n",
      "    possible to return a view.\n",
      "    \n",
      "    See :func:`torch.reshape`\n",
      "    \n",
      "    Args:\n",
      "        shape (tuple of ints or int...): the desired shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.Tensor.reshape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto gradients of tensor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2., -1.],\n",
      "        [ 1.,  1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2., -1.], [1., 1.]], requires_grad=True, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n",
      "tensor(7., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_pow = x.pow(2)\n",
    "print(x_pow)\n",
    "\n",
    "out = x_pow.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 500)\n",
    "y = torch.rand(500, 700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cuda, y_cuda = x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.6893e-01, 1.2702e-01, 6.3977e-01, 4.8693e-01, 6.6883e-01, 5.2767e-01,\n",
       "          2.9266e-01, 4.8771e-01, 4.0668e-02, 7.2024e-01, 6.3549e-01, 3.8948e-02,\n",
       "          7.7164e-01, 5.2639e-01, 2.1093e-01, 3.5002e-02, 4.6697e-01, 1.4309e-01,\n",
       "          3.0893e-01, 5.6080e-01, 7.2982e-01, 1.5240e-01, 8.9344e-01, 3.4833e-01,\n",
       "          2.8919e-01, 1.8048e-01, 4.0390e-01, 3.5767e-01, 1.8823e-01, 5.2853e-01,\n",
       "          8.6416e-01, 6.1343e-01, 1.5101e-01, 3.2771e-01, 4.1272e-01, 9.3814e-01,\n",
       "          9.7716e-01, 1.8983e-01, 8.9514e-01, 2.5325e-01, 7.3271e-02, 2.0170e-01,\n",
       "          5.9299e-01, 3.3890e-01, 1.0264e-01, 6.8640e-01, 6.0104e-01, 6.7730e-01,\n",
       "          4.4060e-01, 1.4742e-01, 3.9271e-01, 4.6176e-02, 8.2724e-02, 4.7608e-01,\n",
       "          6.1529e-01, 1.5224e-01, 3.4442e-01, 7.1615e-01, 2.5812e-01, 1.2411e-01,\n",
       "          2.1141e-01, 8.6101e-01, 8.1167e-01, 7.5149e-01, 3.6057e-01, 2.9602e-01,\n",
       "          2.4711e-01, 5.3215e-01, 1.3992e-01, 2.1833e-01, 8.5877e-01, 4.9608e-01,\n",
       "          9.0846e-01, 2.6996e-01, 2.1393e-01, 1.1014e-01, 5.2837e-01, 9.4578e-02,\n",
       "          6.7621e-02, 1.3281e-01, 2.2043e-02, 1.8787e-01, 8.7766e-02, 2.2558e-01,\n",
       "          9.4984e-01, 9.9621e-01, 2.9964e-01, 8.7082e-01, 7.8707e-02, 6.6511e-01,\n",
       "          5.1176e-01, 7.4362e-01, 7.1932e-01, 4.5505e-02, 3.9722e-03, 7.7796e-01,\n",
       "          1.8514e-02, 2.1201e-01, 7.2534e-01, 4.5497e-01, 4.4470e-01, 7.5049e-01,\n",
       "          2.4686e-01, 9.8423e-01, 9.2353e-02, 3.3329e-01, 9.7609e-01, 5.7043e-01,\n",
       "          1.0267e-01, 6.6241e-01, 1.3832e-01, 1.4831e-01, 2.4091e-01, 1.3612e-01,\n",
       "          2.3686e-01, 9.3462e-01, 8.4590e-01, 2.4115e-01, 6.4250e-01, 3.0964e-01,\n",
       "          8.7877e-01, 7.0520e-01, 1.9142e-01, 2.8943e-01, 1.6857e-01, 1.4093e-01,\n",
       "          7.7226e-01, 1.3715e-01, 6.9830e-01, 9.6933e-01, 6.7918e-01, 6.0967e-01,\n",
       "          6.0286e-01, 9.9566e-01, 8.7452e-01, 8.3505e-01, 7.9929e-01, 3.3666e-01,\n",
       "          7.4600e-01, 3.7011e-01, 7.4605e-01, 5.9543e-01, 1.6713e-01, 6.9169e-01,\n",
       "          3.7337e-01, 1.1483e-01, 8.9155e-01, 9.5354e-02, 4.0340e-01, 7.6413e-01,\n",
       "          2.9042e-01, 6.1691e-01, 9.9574e-01, 3.7542e-01, 3.4663e-01, 6.3260e-01,\n",
       "          7.5941e-01, 7.8422e-01, 1.7467e-01, 5.5063e-01, 9.1102e-01, 5.5937e-01,\n",
       "          7.1061e-01, 8.1744e-01, 8.8994e-01, 2.7191e-01, 5.6805e-01, 3.7107e-01,\n",
       "          6.8004e-01, 6.6071e-02, 1.6108e-01, 3.6578e-01, 5.8784e-01, 9.5212e-01,\n",
       "          2.5060e-01, 8.6067e-01, 8.8377e-01, 4.0015e-01, 4.6099e-01, 5.8092e-01,\n",
       "          8.7600e-01, 4.5553e-01, 8.5373e-01, 8.7106e-01, 9.2048e-01, 1.5863e-01,\n",
       "          5.0851e-01, 3.9772e-01, 2.3752e-01, 5.0217e-01, 1.4036e-01, 7.3650e-01,\n",
       "          6.7787e-01, 6.7039e-01, 9.4809e-01, 3.0828e-01, 2.9211e-01, 2.0763e-01,\n",
       "          8.0388e-01, 4.3124e-02, 3.5293e-01, 8.9879e-01, 5.1348e-01, 2.1301e-01,\n",
       "          8.1431e-01, 7.8188e-02, 9.7725e-01, 6.8609e-01, 8.9101e-02, 2.0653e-01,\n",
       "          9.2549e-01, 5.6142e-01, 8.7703e-01, 1.4165e-01, 8.5942e-01, 9.8729e-01,\n",
       "          3.0647e-01, 5.7047e-01, 1.1248e-01, 8.0640e-01, 7.0937e-01, 4.0692e-01,\n",
       "          5.6221e-01, 1.5513e-01, 6.7534e-01, 8.0974e-01, 1.8256e-01, 1.9258e-01,\n",
       "          6.3486e-02, 8.3198e-01, 2.6626e-01, 3.3679e-01, 2.1786e-01, 4.6922e-01,\n",
       "          9.8078e-01, 8.9366e-01, 5.2081e-01, 2.4055e-01, 5.4842e-02, 1.3330e-01,\n",
       "          2.9207e-01, 4.8713e-01, 4.7787e-01, 5.6736e-02, 3.9336e-01, 8.7841e-01,\n",
       "          8.2333e-01, 6.6254e-01, 4.5080e-01, 3.2938e-02, 3.9933e-01, 6.9101e-01,\n",
       "          9.6582e-01, 6.1984e-01, 5.0640e-01, 4.1230e-01, 7.9331e-01, 5.9914e-01,\n",
       "          1.7184e-01, 6.6998e-01, 5.4611e-01, 1.5255e-01, 4.6821e-01, 6.0461e-01,\n",
       "          9.4917e-01, 1.2602e-01, 8.9920e-01, 9.5749e-02, 8.1819e-01, 1.8051e-01,\n",
       "          4.5150e-01, 5.4291e-01, 5.9075e-01, 5.4244e-01, 4.7055e-01, 4.0117e-01,\n",
       "          1.7106e-01, 3.0834e-01, 3.6636e-01, 6.7930e-01, 8.0117e-03, 1.4436e-01,\n",
       "          8.8409e-01, 5.6238e-01, 7.1587e-01, 3.5858e-01, 6.5370e-01, 2.4352e-01,\n",
       "          6.4935e-01, 8.5158e-01, 7.2687e-01, 3.4999e-01, 8.1026e-02, 5.7193e-02,\n",
       "          4.5232e-01, 8.6513e-01, 9.6141e-01, 4.1771e-01, 8.3316e-01, 9.0689e-01,\n",
       "          4.4748e-01, 1.2678e-01, 1.7739e-01, 1.1380e-02, 4.9909e-01, 4.7984e-02,\n",
       "          6.8534e-01, 9.1620e-01, 2.0075e-01, 6.4193e-01, 8.1869e-01, 6.4609e-01,\n",
       "          9.5786e-01, 4.1590e-01, 4.1286e-01, 5.7346e-01, 6.4735e-01, 3.5891e-01,\n",
       "          4.4570e-01, 4.3987e-01, 3.5848e-01, 6.9951e-01, 1.6330e-01, 5.5902e-01,\n",
       "          8.3961e-01, 4.3143e-01, 2.6786e-01, 8.1823e-01, 6.8610e-01, 8.0609e-01,\n",
       "          5.0629e-01, 3.9240e-01, 1.5664e-01, 9.5305e-01, 9.7352e-01, 9.2618e-01,\n",
       "          7.2522e-01, 4.3076e-01, 3.2213e-01, 9.6251e-01, 7.4632e-01, 4.7118e-01,\n",
       "          4.6442e-01, 2.1195e-01, 8.2907e-01, 5.6732e-01, 6.1261e-01, 7.6394e-01,\n",
       "          7.1212e-01, 4.3489e-01, 4.9488e-01, 1.5749e-01, 1.5567e-01, 5.1536e-01,\n",
       "          6.1813e-01, 6.8302e-01, 7.5924e-01, 7.8621e-01, 9.4356e-01, 8.0487e-01,\n",
       "          9.5702e-01, 9.7763e-01, 3.9366e-02, 9.9621e-01, 3.2515e-02, 3.0421e-01,\n",
       "          7.1875e-01, 1.2005e-01, 5.5198e-01, 9.6159e-01, 5.4745e-01, 2.5631e-02,\n",
       "          1.8960e-01, 6.6291e-01, 3.3701e-01, 3.7290e-01, 7.4567e-01, 2.5668e-01,\n",
       "          6.6990e-02, 7.9328e-01, 4.0361e-01, 8.6908e-01, 4.4403e-01, 6.2523e-01,\n",
       "          4.1003e-01, 4.2568e-01, 9.4825e-01, 3.2135e-01, 2.6936e-01, 8.3913e-01,\n",
       "          8.1732e-01, 5.9995e-01, 6.3273e-01, 3.5560e-01, 7.1383e-01, 3.9492e-01,\n",
       "          7.7317e-01, 4.6047e-01, 3.3601e-01, 4.3483e-01, 8.7267e-01, 5.0163e-01,\n",
       "          4.9839e-01, 1.0801e-01, 3.7927e-01, 8.9998e-01, 5.9061e-02, 2.9699e-02,\n",
       "          3.6963e-01, 7.1861e-01, 7.0047e-01, 4.7164e-01, 6.8175e-01, 2.2952e-01,\n",
       "          5.1547e-01, 6.7346e-01, 7.2438e-01, 1.9438e-01, 8.1844e-01, 9.0238e-01,\n",
       "          2.6642e-01, 1.9843e-01, 6.5224e-01, 2.6927e-01, 4.7615e-02, 8.0597e-04,\n",
       "          2.7282e-01, 7.0368e-02, 3.8672e-01, 3.1128e-01, 3.1032e-01, 5.8401e-01,\n",
       "          1.4826e-01, 5.3867e-01, 4.6110e-01, 9.4701e-01, 9.7966e-01, 7.2049e-01,\n",
       "          1.2150e-01, 4.9735e-01, 8.8467e-01, 9.1615e-02, 8.6028e-01, 3.8347e-02,\n",
       "          5.6407e-01, 7.0020e-01, 6.0785e-01, 9.8735e-01, 7.5013e-01, 6.1012e-01,\n",
       "          1.0937e-01, 9.1032e-01, 8.9508e-01, 1.6667e-01, 3.3438e-01, 4.9674e-01,\n",
       "          3.4470e-01, 5.9567e-01, 5.7694e-03, 9.4865e-01, 1.2799e-01, 6.4088e-01,\n",
       "          5.9952e-01, 9.4156e-01, 4.2656e-01, 1.9972e-02, 6.0227e-01, 3.4856e-01,\n",
       "          7.1736e-01, 9.2869e-01, 3.4919e-01, 7.1887e-01, 2.7145e-01, 2.4328e-01,\n",
       "          3.7291e-01, 5.1186e-01, 6.7885e-01, 5.7176e-01, 6.5468e-01, 6.7815e-01,\n",
       "          2.1275e-01, 9.2643e-01, 5.8258e-01, 2.8863e-02, 5.8588e-02, 6.8403e-02,\n",
       "          9.6747e-01, 3.0864e-01, 8.0840e-01, 8.4429e-01, 1.5966e-01, 7.7105e-01,\n",
       "          2.9952e-01, 7.4197e-01, 1.3853e-01, 6.9690e-01, 9.5390e-01, 9.6298e-01,\n",
       "          4.1742e-01, 7.2578e-01]], device='cuda:0'),\n",
       " tensor([[0.2352, 0.4948, 0.5687,  ..., 0.7839, 0.8811, 0.7506],\n",
       "         [0.4983, 0.7948, 0.9056,  ..., 0.2358, 0.3446, 0.9836],\n",
       "         [0.6240, 0.5507, 0.8770,  ..., 0.4051, 0.3718, 0.3051],\n",
       "         ...,\n",
       "         [0.5123, 0.1494, 0.4721,  ..., 0.7704, 0.0287, 0.4014],\n",
       "         [0.1086, 0.1494, 0.7795,  ..., 0.5416, 0.3257, 0.3577],\n",
       "         [0.6730, 0.3396, 0.3419,  ..., 0.0988, 0.7372, 0.2453]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cuda, y_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.7 µs ± 3.86 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = (x_cuda @ y_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu, y_cpu = x.cpu(), y.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.6893e-01, 1.2702e-01, 6.3977e-01, 4.8693e-01, 6.6883e-01, 5.2767e-01,\n",
       "          2.9266e-01, 4.8771e-01, 4.0668e-02, 7.2024e-01, 6.3549e-01, 3.8948e-02,\n",
       "          7.7164e-01, 5.2639e-01, 2.1093e-01, 3.5002e-02, 4.6697e-01, 1.4309e-01,\n",
       "          3.0893e-01, 5.6080e-01, 7.2982e-01, 1.5240e-01, 8.9344e-01, 3.4833e-01,\n",
       "          2.8919e-01, 1.8048e-01, 4.0390e-01, 3.5767e-01, 1.8823e-01, 5.2853e-01,\n",
       "          8.6416e-01, 6.1343e-01, 1.5101e-01, 3.2771e-01, 4.1272e-01, 9.3814e-01,\n",
       "          9.7716e-01, 1.8983e-01, 8.9514e-01, 2.5325e-01, 7.3271e-02, 2.0170e-01,\n",
       "          5.9299e-01, 3.3890e-01, 1.0264e-01, 6.8640e-01, 6.0104e-01, 6.7730e-01,\n",
       "          4.4060e-01, 1.4742e-01, 3.9271e-01, 4.6176e-02, 8.2724e-02, 4.7608e-01,\n",
       "          6.1529e-01, 1.5224e-01, 3.4442e-01, 7.1615e-01, 2.5812e-01, 1.2411e-01,\n",
       "          2.1141e-01, 8.6101e-01, 8.1167e-01, 7.5149e-01, 3.6057e-01, 2.9602e-01,\n",
       "          2.4711e-01, 5.3215e-01, 1.3992e-01, 2.1833e-01, 8.5877e-01, 4.9608e-01,\n",
       "          9.0846e-01, 2.6996e-01, 2.1393e-01, 1.1014e-01, 5.2837e-01, 9.4578e-02,\n",
       "          6.7621e-02, 1.3281e-01, 2.2043e-02, 1.8787e-01, 8.7766e-02, 2.2558e-01,\n",
       "          9.4984e-01, 9.9621e-01, 2.9964e-01, 8.7082e-01, 7.8707e-02, 6.6511e-01,\n",
       "          5.1176e-01, 7.4362e-01, 7.1932e-01, 4.5505e-02, 3.9722e-03, 7.7796e-01,\n",
       "          1.8514e-02, 2.1201e-01, 7.2534e-01, 4.5497e-01, 4.4470e-01, 7.5049e-01,\n",
       "          2.4686e-01, 9.8423e-01, 9.2353e-02, 3.3329e-01, 9.7609e-01, 5.7043e-01,\n",
       "          1.0267e-01, 6.6241e-01, 1.3832e-01, 1.4831e-01, 2.4091e-01, 1.3612e-01,\n",
       "          2.3686e-01, 9.3462e-01, 8.4590e-01, 2.4115e-01, 6.4250e-01, 3.0964e-01,\n",
       "          8.7877e-01, 7.0520e-01, 1.9142e-01, 2.8943e-01, 1.6857e-01, 1.4093e-01,\n",
       "          7.7226e-01, 1.3715e-01, 6.9830e-01, 9.6933e-01, 6.7918e-01, 6.0967e-01,\n",
       "          6.0286e-01, 9.9566e-01, 8.7452e-01, 8.3505e-01, 7.9929e-01, 3.3666e-01,\n",
       "          7.4600e-01, 3.7011e-01, 7.4605e-01, 5.9543e-01, 1.6713e-01, 6.9169e-01,\n",
       "          3.7337e-01, 1.1483e-01, 8.9155e-01, 9.5354e-02, 4.0340e-01, 7.6413e-01,\n",
       "          2.9042e-01, 6.1691e-01, 9.9574e-01, 3.7542e-01, 3.4663e-01, 6.3260e-01,\n",
       "          7.5941e-01, 7.8422e-01, 1.7467e-01, 5.5063e-01, 9.1102e-01, 5.5937e-01,\n",
       "          7.1061e-01, 8.1744e-01, 8.8994e-01, 2.7191e-01, 5.6805e-01, 3.7107e-01,\n",
       "          6.8004e-01, 6.6071e-02, 1.6108e-01, 3.6578e-01, 5.8784e-01, 9.5212e-01,\n",
       "          2.5060e-01, 8.6067e-01, 8.8377e-01, 4.0015e-01, 4.6099e-01, 5.8092e-01,\n",
       "          8.7600e-01, 4.5553e-01, 8.5373e-01, 8.7106e-01, 9.2048e-01, 1.5863e-01,\n",
       "          5.0851e-01, 3.9772e-01, 2.3752e-01, 5.0217e-01, 1.4036e-01, 7.3650e-01,\n",
       "          6.7787e-01, 6.7039e-01, 9.4809e-01, 3.0828e-01, 2.9211e-01, 2.0763e-01,\n",
       "          8.0388e-01, 4.3124e-02, 3.5293e-01, 8.9879e-01, 5.1348e-01, 2.1301e-01,\n",
       "          8.1431e-01, 7.8188e-02, 9.7725e-01, 6.8609e-01, 8.9101e-02, 2.0653e-01,\n",
       "          9.2549e-01, 5.6142e-01, 8.7703e-01, 1.4165e-01, 8.5942e-01, 9.8729e-01,\n",
       "          3.0647e-01, 5.7047e-01, 1.1248e-01, 8.0640e-01, 7.0937e-01, 4.0692e-01,\n",
       "          5.6221e-01, 1.5513e-01, 6.7534e-01, 8.0974e-01, 1.8256e-01, 1.9258e-01,\n",
       "          6.3486e-02, 8.3198e-01, 2.6626e-01, 3.3679e-01, 2.1786e-01, 4.6922e-01,\n",
       "          9.8078e-01, 8.9366e-01, 5.2081e-01, 2.4055e-01, 5.4842e-02, 1.3330e-01,\n",
       "          2.9207e-01, 4.8713e-01, 4.7787e-01, 5.6736e-02, 3.9336e-01, 8.7841e-01,\n",
       "          8.2333e-01, 6.6254e-01, 4.5080e-01, 3.2938e-02, 3.9933e-01, 6.9101e-01,\n",
       "          9.6582e-01, 6.1984e-01, 5.0640e-01, 4.1230e-01, 7.9331e-01, 5.9914e-01,\n",
       "          1.7184e-01, 6.6998e-01, 5.4611e-01, 1.5255e-01, 4.6821e-01, 6.0461e-01,\n",
       "          9.4917e-01, 1.2602e-01, 8.9920e-01, 9.5749e-02, 8.1819e-01, 1.8051e-01,\n",
       "          4.5150e-01, 5.4291e-01, 5.9075e-01, 5.4244e-01, 4.7055e-01, 4.0117e-01,\n",
       "          1.7106e-01, 3.0834e-01, 3.6636e-01, 6.7930e-01, 8.0117e-03, 1.4436e-01,\n",
       "          8.8409e-01, 5.6238e-01, 7.1587e-01, 3.5858e-01, 6.5370e-01, 2.4352e-01,\n",
       "          6.4935e-01, 8.5158e-01, 7.2687e-01, 3.4999e-01, 8.1026e-02, 5.7193e-02,\n",
       "          4.5232e-01, 8.6513e-01, 9.6141e-01, 4.1771e-01, 8.3316e-01, 9.0689e-01,\n",
       "          4.4748e-01, 1.2678e-01, 1.7739e-01, 1.1380e-02, 4.9909e-01, 4.7984e-02,\n",
       "          6.8534e-01, 9.1620e-01, 2.0075e-01, 6.4193e-01, 8.1869e-01, 6.4609e-01,\n",
       "          9.5786e-01, 4.1590e-01, 4.1286e-01, 5.7346e-01, 6.4735e-01, 3.5891e-01,\n",
       "          4.4570e-01, 4.3987e-01, 3.5848e-01, 6.9951e-01, 1.6330e-01, 5.5902e-01,\n",
       "          8.3961e-01, 4.3143e-01, 2.6786e-01, 8.1823e-01, 6.8610e-01, 8.0609e-01,\n",
       "          5.0629e-01, 3.9240e-01, 1.5664e-01, 9.5305e-01, 9.7352e-01, 9.2618e-01,\n",
       "          7.2522e-01, 4.3076e-01, 3.2213e-01, 9.6251e-01, 7.4632e-01, 4.7118e-01,\n",
       "          4.6442e-01, 2.1195e-01, 8.2907e-01, 5.6732e-01, 6.1261e-01, 7.6394e-01,\n",
       "          7.1212e-01, 4.3489e-01, 4.9488e-01, 1.5749e-01, 1.5567e-01, 5.1536e-01,\n",
       "          6.1813e-01, 6.8302e-01, 7.5924e-01, 7.8621e-01, 9.4356e-01, 8.0487e-01,\n",
       "          9.5702e-01, 9.7763e-01, 3.9366e-02, 9.9621e-01, 3.2515e-02, 3.0421e-01,\n",
       "          7.1875e-01, 1.2005e-01, 5.5198e-01, 9.6159e-01, 5.4745e-01, 2.5631e-02,\n",
       "          1.8960e-01, 6.6291e-01, 3.3701e-01, 3.7290e-01, 7.4567e-01, 2.5668e-01,\n",
       "          6.6990e-02, 7.9328e-01, 4.0361e-01, 8.6908e-01, 4.4403e-01, 6.2523e-01,\n",
       "          4.1003e-01, 4.2568e-01, 9.4825e-01, 3.2135e-01, 2.6936e-01, 8.3913e-01,\n",
       "          8.1732e-01, 5.9995e-01, 6.3273e-01, 3.5560e-01, 7.1383e-01, 3.9492e-01,\n",
       "          7.7317e-01, 4.6047e-01, 3.3601e-01, 4.3483e-01, 8.7267e-01, 5.0163e-01,\n",
       "          4.9839e-01, 1.0801e-01, 3.7927e-01, 8.9998e-01, 5.9061e-02, 2.9699e-02,\n",
       "          3.6963e-01, 7.1861e-01, 7.0047e-01, 4.7164e-01, 6.8175e-01, 2.2952e-01,\n",
       "          5.1547e-01, 6.7346e-01, 7.2438e-01, 1.9438e-01, 8.1844e-01, 9.0238e-01,\n",
       "          2.6642e-01, 1.9843e-01, 6.5224e-01, 2.6927e-01, 4.7615e-02, 8.0597e-04,\n",
       "          2.7282e-01, 7.0368e-02, 3.8672e-01, 3.1128e-01, 3.1032e-01, 5.8401e-01,\n",
       "          1.4826e-01, 5.3867e-01, 4.6110e-01, 9.4701e-01, 9.7966e-01, 7.2049e-01,\n",
       "          1.2150e-01, 4.9735e-01, 8.8467e-01, 9.1615e-02, 8.6028e-01, 3.8347e-02,\n",
       "          5.6407e-01, 7.0020e-01, 6.0785e-01, 9.8735e-01, 7.5013e-01, 6.1012e-01,\n",
       "          1.0937e-01, 9.1032e-01, 8.9508e-01, 1.6667e-01, 3.3438e-01, 4.9674e-01,\n",
       "          3.4470e-01, 5.9567e-01, 5.7694e-03, 9.4865e-01, 1.2799e-01, 6.4088e-01,\n",
       "          5.9952e-01, 9.4156e-01, 4.2656e-01, 1.9972e-02, 6.0227e-01, 3.4856e-01,\n",
       "          7.1736e-01, 9.2869e-01, 3.4919e-01, 7.1887e-01, 2.7145e-01, 2.4328e-01,\n",
       "          3.7291e-01, 5.1186e-01, 6.7885e-01, 5.7176e-01, 6.5468e-01, 6.7815e-01,\n",
       "          2.1275e-01, 9.2643e-01, 5.8258e-01, 2.8863e-02, 5.8588e-02, 6.8403e-02,\n",
       "          9.6747e-01, 3.0864e-01, 8.0840e-01, 8.4429e-01, 1.5966e-01, 7.7105e-01,\n",
       "          2.9952e-01, 7.4197e-01, 1.3853e-01, 6.9690e-01, 9.5390e-01, 9.6298e-01,\n",
       "          4.1742e-01, 7.2578e-01]]),\n",
       " tensor([[0.2352, 0.4948, 0.5687,  ..., 0.7839, 0.8811, 0.7506],\n",
       "         [0.4983, 0.7948, 0.9056,  ..., 0.2358, 0.3446, 0.9836],\n",
       "         [0.6240, 0.5507, 0.8770,  ..., 0.4051, 0.3718, 0.3051],\n",
       "         ...,\n",
       "         [0.5123, 0.1494, 0.4721,  ..., 0.7704, 0.0287, 0.4014],\n",
       "         [0.1086, 0.1494, 0.7795,  ..., 0.5416, 0.3257, 0.3577],\n",
       "         [0.6730, 0.3396, 0.3419,  ..., 0.0988, 0.7372, 0.2453]]))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cpu, y_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.2 µs ± 13.3 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z=(x_cpu @ y_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_np = np.random.random((1, 500))\n",
    "y_np = np.random.random((500, 700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 µs ± 46.8 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = np.matmul(x_np, y_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cuda.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cuda.type(torch.float16).dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
