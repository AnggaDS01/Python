{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary as summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6],\n",
    "    [7, 8],\n",
    "])\n",
    "\n",
    "y = torch.tensor([\n",
    "    [3],\n",
    "    [7],\n",
    "    [11],\n",
    "    [15],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6],\n",
       "         [7, 8]]),\n",
       " tensor([[ 3],\n",
       "         [ 7],\n",
       "         [11],\n",
       "         [15]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:\n",
      "X: torch.int64 | y: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f'type:\\nX: {X.dtype} | y: {y.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_float = X.type(torch.float32)\n",
    "y_float = y.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.],\n",
       "         [5., 6.],\n",
       "         [7., 8.]]),\n",
       " tensor([[ 3.],\n",
       "         [ 7.],\n",
       "         [11.],\n",
       "         [15.]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_float, y_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type:\n",
      "X: torch.float32 | y: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f'type:\\nX: {X_float.dtype} | y: {y_float.dtype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_device = X_float.to(device)\n",
    "y_device = y_float.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.],\n",
       "         [5., 6.],\n",
       "         [7., 8.]], device='cuda:0'),\n",
       " tensor([[ 3.],\n",
       "         [ 7.],\n",
       "         [11.],\n",
       "         [15.]], device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_device, y_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6797,  0.1070,  1.6480],\n",
       "        [-1.4176,  0.9636,  4.1210],\n",
       "        [-2.1554,  1.8201,  6.5939],\n",
       "        [-2.8932,  2.6767,  9.0669]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer= nn.Linear(in_features=2, out_features=3, device=device)(X_device)\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(in_features=2, out_features=16)\n",
    "        self.relu_1 = nn.ReLU()\n",
    "        self.dense_2 = nn.Linear(in_features=16, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.relu_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "simpleNN = SimpleNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Linear: 1-1                            [-1, 16]                  48\n",
      "├─ReLU: 1-2                              [-1, 16]                  --\n",
      "├─Linear: 1-3                            [-1, 1]                   17\n",
      "==========================================================================================\n",
      "Total params: 65\n",
      "Trainable params: 65\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.00\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Linear: 1-1                            [-1, 16]                  48\n",
       "├─ReLU: 1-2                              [-1, 16]                  --\n",
       "├─Linear: 1-3                            [-1, 1]                   17\n",
       "==========================================================================================\n",
       "Total params: 65\n",
       "Trainable params: 65\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.summary(simpleNN, torch.zeros(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS = nn.MSELoss()\n",
    "OPTIMIZER = torch.optim.Adam(simpleNN.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(149.7222, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_loss = simpleNN(X_device)\n",
    "loss_value = LOSS(y_loss, y_device)\n",
    "print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Loss: 74.03160095214844\n",
      "Epoch 200, Loss: 27.51620101928711\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    OPTIMIZER.zero_grad()\n",
    "    \n",
    "    y_outputs = simpleNN(X_device)\n",
    "    loss_value = LOSS(y_outputs, y_device)\n",
    "    loss_value.backward()\n",
    "    \n",
    "    OPTIMIZER.step()\n",
    "\n",
    "    loss_history.append(loss_value.item())\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([148, 2]) torch.Size([148, 1])\n",
      "tensor([[ 2., 10.],\n",
      "        [ 6., 16.],\n",
      "        [10., 22.],\n",
      "        [14., 28.],\n",
      "        [18., 34.]]) tensor([[22],\n",
      "        [38],\n",
      "        [54],\n",
      "        [70],\n",
      "        [86]])\n"
     ]
    }
   ],
   "source": [
    "n_data = 900\n",
    "step_x1 = 4\n",
    "step_x2 = 6\n",
    "start_x1 = 2\n",
    "start_x2 = 10\n",
    "\n",
    "# Menghitung jumlah elemen agar kedua tensor memiliki panjang yang sama\n",
    "len_x1 = (n_data - start_x1) // step_x1\n",
    "len_x2 = (n_data - start_x2) // step_x2\n",
    "min_len = min(len_x1, len_x2)\n",
    "\n",
    "# Membuat tensor X dan Y dengan panjang yang sama\n",
    "X_1 = torch.arange(start_x1, start_x1 + min_len * step_x1, step_x1).reshape(-1, 1)\n",
    "X_2 = torch.arange(start_x2, start_x2 + min_len * step_x2, step_x2).reshape(-1, 1)\n",
    "\n",
    "Y = X_1 + 2 * X_2 \n",
    "\n",
    "# Menggabungkan kedua tensor\n",
    "X = torch.concat((X_1, X_2), dim=1).type(torch.float32)\n",
    "print(X.shape, Y.shape)\n",
    "print(X[:5], Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x_float = self.x.type(torch.float32)\n",
    "        y_float = self.y.type(torch.float32)\n",
    "\n",
    "        x_clone = x_float.clone().detach()\n",
    "        y_clone = y_float.clone().detach()\n",
    "\n",
    "        X_grad = x_clone.requires_grad_(True)\n",
    "        y_grad = y_clone.requires_grad_(True)\n",
    "\n",
    "        return X_grad[index].to(device), y_grad[index].to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "torch_data = MyDataset(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2., 10.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([22.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 6., 16.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([38.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([10., 22.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([54.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([14., 28.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([70.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([18., 34.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([86.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([22., 40.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([102.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([26., 46.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([118.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([30., 52.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([134.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([34., 58.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([150.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([38., 64.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([166.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([42., 70.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([182.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([46., 76.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([198.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([50., 82.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([214.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([54., 88.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([230.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([58., 94.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([246.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 62., 100.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([262.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 66., 106.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([278.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 70., 112.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([294.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 74., 118.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([310.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 78., 124.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([326.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 82., 130.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([342.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 86., 136.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([358.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 90., 142.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([374.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 94., 148.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([390.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([ 98., 154.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([406.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([102., 160.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([422.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([106., 166.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([438.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([110., 172.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([454.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([114., 178.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([470.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([118., 184.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([486.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([122., 190.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([502.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([126., 196.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([518.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([130., 202.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([534.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([134., 208.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([550.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([138., 214.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([566.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([142., 220.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([582.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([146., 226.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([598.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([150., 232.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([614.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([154., 238.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([630.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([158., 244.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([646.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([162., 250.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([662.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([166., 256.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([678.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([170., 262.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([694.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([174., 268.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([710.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([178., 274.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([726.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([182., 280.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([742.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([186., 286.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([758.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([190., 292.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([774.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([194., 298.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([790.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([198., 304.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([806.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([202., 310.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([822.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([206., 316.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([838.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([210., 322.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([854.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([214., 328.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([870.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([218., 334.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([886.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([222., 340.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([902.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([226., 346.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([918.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([230., 352.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([934.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([234., 358.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([950.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([238., 364.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([966.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([242., 370.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([982.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([246., 376.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([998.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([250., 382.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1014.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([254., 388.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1030.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([258., 394.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1046.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([262., 400.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1062.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([266., 406.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1078.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([270., 412.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1094.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([274., 418.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1110.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([278., 424.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1126.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([282., 430.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1142.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([286., 436.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1158.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([290., 442.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1174.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([294., 448.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1190.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([298., 454.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1206.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([302., 460.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1222.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([306., 466.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1238.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([310., 472.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1254.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([314., 478.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1270.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([318., 484.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1286.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([322., 490.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1302.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([326., 496.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1318.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([330., 502.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1334.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([334., 508.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1350.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([338., 514.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1366.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([342., 520.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1382.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([346., 526.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1398.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([350., 532.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1414.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([354., 538.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1430.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([358., 544.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1446.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([362., 550.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1462.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([366., 556.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1478.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([370., 562.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1494.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([374., 568.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1510.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([378., 574.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1526.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([382., 580.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1542.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([386., 586.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1558.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([390., 592.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1574.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([394., 598.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1590.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([398., 604.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1606.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([402., 610.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1622.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([406., 616.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1638.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([410., 622.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1654.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([414., 628.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1670.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([418., 634.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1686.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([422., 640.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1702.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([426., 646.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1718.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([430., 652.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1734.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([434., 658.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1750.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([438., 664.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1766.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([442., 670.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1782.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([446., 676.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1798.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([450., 682.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1814.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([454., 688.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1830.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([458., 694.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1846.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([462., 700.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1862.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([466., 706.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1878.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([470., 712.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1894.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([474., 718.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1910.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([478., 724.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1926.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([482., 730.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1942.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([486., 736.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1958.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([490., 742.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1974.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([494., 748.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1990.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([498., 754.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2006.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([502., 760.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2022.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([506., 766.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2038.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([510., 772.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2054.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([514., 778.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2070.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([518., 784.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2086.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([522., 790.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2102.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([526., 796.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2118.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([530., 802.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2134.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([534., 808.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2150.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([538., 814.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2166.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([542., 820.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2182.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([546., 826.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2198.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([550., 832.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2214.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([554., 838.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2230.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([558., 844.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2246.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([562., 850.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2262.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([566., 856.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2278.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([570., 862.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2294.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([574., 868.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2310.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([578., 874.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2326.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([582., 880.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2342.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([586., 886.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2358.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "tensor([590., 892.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([2374.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for feature, label in torch_data:\n",
    "    print(feature, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 148\n",
      "train total: 133\n",
      "valid total: 15\n",
      "total train + valid: 148\n"
     ]
    }
   ],
   "source": [
    "train_split = int(len(torch_data) * .9)\n",
    "valid_split = len(torch_data) - int(len(torch_data) * .9)\n",
    "\n",
    "print(f'total: {len(torch_data)}')\n",
    "print(f'train total: {train_split}')\n",
    "print(f'valid total: {valid_split}')\n",
    "print(f'total train + valid: {train_split + valid_split}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(torch_data, [train_split, valid_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 15)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([306., 466.], device='cuda:0', grad_fn=<ToCopyBackward0>) tensor([1238.], device='cuda:0', grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for feature, label in train_set:\n",
    "    print(feature, label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_data_train_pipeline = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=False)\n",
    "\n",
    "torch_data_valid_pipeline = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4758.,  7144.],\n",
      "        [ 1878.,  2824.],\n",
      "        [18906., 28366.],\n",
      "        [ 7966., 11956.]], device='cuda:0', grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for feature, label in torch_data_train_pipeline:\n",
    "    print(feature)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 18.223663330078125\n",
      "Epoch 2, Loss: 0.13342314958572388\n",
      "Epoch 3, Loss: 0.000689713517203927\n",
      "Epoch 4, Loss: 0.000592874304857105\n",
      "Epoch 5, Loss: 0.0005092871142551303\n",
      "Epoch 6, Loss: 0.0004273323866073042\n",
      "Epoch 7, Loss: 0.0003505703352857381\n",
      "Epoch 8, Loss: 0.0002816477499436587\n",
      "Epoch 9, Loss: 0.00022221647668629885\n",
      "Epoch 10, Loss: 0.00017310766270384192\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch, (feature, label) in enumerate(torch_data_pipeline, 1):\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        y_outputs = simpleNN(X_device)\n",
    "        loss_value = LOSS(y_outputs, y_device)\n",
    "        loss_value.backward()\n",
    "        \n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        loss_history.append(loss_value.item())\n",
    "\n",
    "        if batch % len(torch_data_pipeline) == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
